{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short notebook 2\n",
    "\n",
    "**[group 94] Gosling slayers**  \n",
    "Andrea Grimsdatter Stallvik. Studet number: 528429  \n",
    "Taheera Ahmed. Student number: 491658\n",
    "\n",
    "This notebook generated the submission with the filename `11_nov_1055.csv`, which recieved a public score of 146.844926 on the leaderboard.\n",
    "\n",
    "In this attempt we are using the mean value of featuers per hour + some extra hourly min/max values for select features during data processing, adding some interaction features, adding some rolling averages for some features for 24 hr, and dropping some columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note:* The packages and version numbers used for this notebook are:  \n",
    "pandas==1.5.3  \n",
    "numpy==1.26.0  \n",
    "scikit-learn==1.2.2  \n",
    "xgboost==1.7.6  \n",
    "catboost==1.1.1  \n",
    "seaborn==0.12.2  \n",
    "matplotlib==3.8.0  \n",
    "notebook==7.0.4  \n",
    "\n",
    "With python version 3.10.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for data loading / data processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for:\n",
    "- loading data\n",
    "- parsing submission\n",
    "- ...\n",
    "\n",
    "NOTE: all functions file should be pasted into the long notebook before submission.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
    "    \n",
    "\n",
    "def load_data(mean=False, mean_stats = False, roll_avg=False, remove_out=False):\n",
    "    \"\"\"Loads data, drops rows that have missing values for the target variable.\"\"\"\n",
    "\n",
    "    # --- Check if files exist ---\n",
    "    file_paths = [\n",
    "        'data/A/train_targets.parquet',\n",
    "        'data/B/train_targets.parquet',\n",
    "        'data/C/train_targets.parquet',\n",
    "        'data/A/X_train_estimated.parquet',\n",
    "        'data/B/X_train_estimated.parquet',\n",
    "        'data/C/X_train_estimated.parquet',\n",
    "        'data/A/X_train_observed.parquet',\n",
    "        'data/B/X_train_observed.parquet',\n",
    "        'data/C/X_train_observed.parquet',\n",
    "    ]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        check_file_exists(file_path)\n",
    "\n",
    "    # ---- load data from files ----\n",
    "    train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    # --- get features for each hour before concatinating ---\n",
    "    if mean:\n",
    "        X_train_observed_a = get_hourly_mean(X_train_observed_a)\n",
    "        X_train_observed_b = get_hourly_mean(X_train_observed_b)\n",
    "        X_train_observed_c = get_hourly_mean(X_train_observed_c)\n",
    "\n",
    "        X_train_estimated_a = get_hourly_mean(X_train_estimated_a)\n",
    "        X_train_estimated_b = get_hourly_mean(X_train_estimated_b)\n",
    "        X_train_estimated_c = get_hourly_mean(X_train_estimated_c)\n",
    "    elif mean_stats:\n",
    "        X_train_observed_a = get_hourly_stats(X_train_observed_a)\n",
    "        X_train_observed_b = get_hourly_stats(X_train_observed_b)\n",
    "        X_train_observed_c = get_hourly_stats(X_train_observed_c)\n",
    "\n",
    "        X_train_estimated_a = get_hourly_stats(X_train_estimated_a)\n",
    "        X_train_estimated_b = get_hourly_stats(X_train_estimated_b)\n",
    "        X_train_estimated_c = get_hourly_stats(X_train_estimated_c)\n",
    "    else:\n",
    "        X_train_observed_a = get_hourly(X_train_observed_a)\n",
    "        X_train_observed_b = get_hourly(X_train_observed_b)\n",
    "        X_train_observed_c = get_hourly(X_train_observed_c)\n",
    "\n",
    "        X_train_estimated_a = get_hourly(X_train_estimated_a)\n",
    "        X_train_estimated_b = get_hourly(X_train_estimated_b)\n",
    "        X_train_estimated_c = get_hourly(X_train_estimated_c)\n",
    "\n",
    "    X_train_observed_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "\n",
    "    X_train_observed_a[\"estimated_flag\"] = 0\n",
    "    X_train_observed_b[\"estimated_flag\"] = 0\n",
    "    X_train_observed_c[\"estimated_flag\"] = 0\n",
    "    X_train_estimated_a[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_b[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_c[\"estimated_flag\"] = 1\n",
    "\n",
    "    # --- merge observed and estimated data with target data, lining up time-stamps correctly ----\n",
    "    train_obs_a = pd.merge(train_a, X_train_observed_a, on='time', how='inner')\n",
    "    train_obs_b = pd.merge(train_b, X_train_observed_b, on='time', how='inner') # NOTE: 4 missing values for target\n",
    "    train_obs_c = pd.merge(train_c, X_train_observed_c, on='time', how='inner') # NOTE: 6059 missing values for target\n",
    "\n",
    "    train_est_a = pd.merge(train_a, X_train_estimated_a, on='time', how='inner')\n",
    "    train_est_b = pd.merge(train_b, X_train_estimated_b, on='time', how='inner')\n",
    "    train_est_c = pd.merge(train_c, X_train_estimated_c, on='time', how='inner')\n",
    "\n",
    "    data_a = pd.concat([train_obs_a, train_est_a], axis=0, ignore_index=True)\n",
    "    data_b = pd.concat([train_obs_b, train_est_b], axis=0, ignore_index=True)\n",
    "    data_c = pd.concat([train_obs_c, train_est_c], axis=0, ignore_index=True)\n",
    "\n",
    "    # remove rows that the target value is missing from since they will not be useful in model training\n",
    "    data_a = data_a.dropna(subset=['pv_measurement'])\n",
    "    data_b = data_b.dropna(subset=['pv_measurement'])\n",
    "    data_c = data_c.dropna(subset=['pv_measurement'])\n",
    "\n",
    "    # add columnns for rolling average\n",
    "    if roll_avg:\n",
    "        data_a = rolling_average(data_a)\n",
    "        data_b = rolling_average(data_b)\n",
    "        data_c = rolling_average(data_c)\n",
    "\n",
    "    if remove_out:\n",
    "        data_a = remove_ouliers(data_a)\n",
    "        data_b = remove_ouliers(data_b, remove_b_outliers=True)\n",
    "        data_c = remove_ouliers(data_c)\n",
    "\n",
    "    return data_a, data_b, data_c\n",
    "\n",
    "\n",
    "def load_data_interpolate(roll_avg=False, remove_out=False):\n",
    "    \n",
    "    # ---- load data from files ----\n",
    "    train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    X_train_observed_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "\n",
    "    X_train_observed_a[\"estimated_flag\"] = 0\n",
    "    X_train_observed_b[\"estimated_flag\"] = 0\n",
    "    X_train_observed_c[\"estimated_flag\"] = 0\n",
    "    X_train_estimated_a[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_b[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_c[\"estimated_flag\"] = 1\n",
    "\n",
    "    # --- merge observed and estimated data with target data, leave NaN values for pv_measurement ----\n",
    "    train_obs_a = pd.merge(train_a, X_train_observed_a, on='time', how='right')\n",
    "    train_obs_b = pd.merge(train_b, X_train_observed_b, on='time', how='right')\n",
    "    train_obs_c = pd.merge(train_c, X_train_observed_c, on='time', how='right')\n",
    "\n",
    "    train_est_a = pd.merge(train_a, X_train_estimated_a, on='time', how='right')\n",
    "    train_est_b = pd.merge(train_b, X_train_estimated_b, on='time', how='right')\n",
    "    train_est_c = pd.merge(train_c, X_train_estimated_c, on='time', how='right')\n",
    "\n",
    "    data_a = pd.concat([train_obs_a, train_est_a], axis=0, ignore_index=True)\n",
    "    data_b = pd.concat([train_obs_b, train_est_b], axis=0, ignore_index=True)\n",
    "    data_c = pd.concat([train_obs_c, train_est_c], axis=0, ignore_index=True)\n",
    "\n",
    "    data_a = fill_pv_values(data_a)\n",
    "    data_b = fill_pv_values(data_b)\n",
    "    data_c = fill_pv_values(data_c)\n",
    "\n",
    "    # add columnns for rolling average\n",
    "    if roll_avg:\n",
    "        data_a = rolling_average(data_a)\n",
    "        data_b = rolling_average(data_b)\n",
    "        data_c = rolling_average(data_c)\n",
    "\n",
    "    if remove_out:\n",
    "        data_a = remove_ouliers(data_a)\n",
    "        data_b = remove_ouliers(data_b, remove_b_outliers=True)\n",
    "        data_c = remove_ouliers(data_c)\n",
    "\n",
    "    return data_a, data_b, data_c\n",
    "\n",
    "\n",
    "def remove_ouliers(data, remove_b_outliers = False):\n",
    "    \"\"\"Removes datapoints that have been static over long stretches (likely due to sensor error!).\"\"\"\n",
    "\n",
    "    threshold = 0.01\n",
    "    window_size = 24\n",
    "\n",
    "    # Calculate standard deviation for each window\n",
    "    std_dev = data['pv_measurement'].rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "    # Identify constant stretches and create a mask to filter out these points\n",
    "    constant_mask = std_dev < threshold\n",
    "\n",
    "    # Filter out constant stretches from the data\n",
    "    filtered_data = data[~constant_mask]\n",
    "\n",
    "    if remove_b_outliers:\n",
    "        \"removing some extra outliers\"\n",
    "        # Remove rows where pv_measurement > 100 and diffuse_rad:W < 30\n",
    "        filtered_data = filtered_data[~((filtered_data[\"pv_measurement\"] > 100) & (filtered_data[\"diffuse_rad:W\"] < 30))]\n",
    "\n",
    "        # Remove rows where pv_measurement > 200 and diffuse_rad:W < 40\n",
    "        filtered_data = filtered_data[~((filtered_data[\"pv_measurement\"] > 200) & (filtered_data[\"diffuse_rad:W\"] < 40))]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def get_hourly(df):\n",
    "    \n",
    "    df[\"minute\"] = df[\"time\"].dt.minute\n",
    "\n",
    "    min_vals = df[\"minute\"].unique()\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for value in min_vals:\n",
    "        filtered_data = df[df['minute'] == value].copy()\n",
    "        filtered_data.drop(columns=['minute'], inplace=True)\n",
    "        filtered_data.columns = [f'{col}_{value}' for col in filtered_data.columns]\n",
    "        filtered_data[\"time_hour\"] = filtered_data[\"time_\"+str(value)].apply(lambda x: x.floor('H'))\n",
    "        df_list.append(filtered_data)\n",
    "\n",
    "    # merge df's on hourly time\n",
    "    merged_df = pd.merge(df_list[0], df_list[1], on=\"time_hour\")\n",
    "    for df in df_list[2:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=\"time_hour\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def get_hourly_mean(df):\n",
    "    \"\"\"Returns a dataframe in which \"\"\"\n",
    "    \n",
    "    # get a column for the start hour\n",
    "    df[\"time_hour\"] = df[\"time\"].apply(lambda x: x.floor('H'))\n",
    "    \n",
    "    # get the mean value for the entire hour\n",
    "    mean_df = df.groupby('time_hour').agg('mean').reset_index()\n",
    "\n",
    "    return mean_df\n",
    "\n",
    "def get_hourly_stats(df, important_features = ['clear_sky_energy_1h:J','clear_sky_rad:W', 'direct_rad:W', 'direct_rad_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'total_cloud_cover:p', 'sun_elevation:d']):\n",
    "    \"\"\"Returns a dataframe with hourly mean for all features and min/max for selected important features.\"\"\"\n",
    "    \n",
    "    # get a column for the start hour\n",
    "    df[\"time_hour\"] = df[\"time\"].apply(lambda x: x.floor('H'))\n",
    "    \n",
    "    # get the mean value for all features for the entire hour\n",
    "    mean_df = df.groupby('time_hour').agg('mean', numeric_only=True).reset_index()\n",
    "\n",
    "    # get min and max for selected important features\n",
    "    min_max_df = df.groupby('time_hour')[important_features].agg(['min', 'max'], numeric_only=True).reset_index()\n",
    "\n",
    "    min_max_df.columns = ['{}_{}'.format(col[0], col[1]) if col[1] != 'time_hour' else col[1] for col in min_max_df.columns]\n",
    "    min_max_df.rename(columns={\"time_hour_\":\"time_hour\"}, inplace=True)\n",
    "\n",
    "    # merge the mean and min/max dataframes on the time_hour column\n",
    "    result_df = pd.merge(mean_df, min_max_df, on='time_hour')\n",
    "\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def fill_pv_values(df):\n",
    "    \"\"\"Fill the pv-values to account for the entire hour\"\"\"\n",
    "\n",
    "    # get a column for the start hour\n",
    "    df[\"time_hour\"] = df[\"time\"].apply(lambda x: x.floor('H'))\n",
    "\n",
    "    # Calculate linear interpolation for each hour\n",
    "    df['pv_measurement'] = df.groupby('time_hour')['pv_measurement'].transform(lambda x: x.interpolate())\n",
    "\n",
    "    # Drop the temporary column used for grouping\n",
    "    df = df.drop(columns=[\"time_hour\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def rolling_average(df, window_size=24,features=['clear_sky_energy_1h:J','clear_sky_rad:W', 'direct_rad:W', 'direct_rad_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'total_cloud_cover:p', 'sun_elevation:d']):\n",
    "    \n",
    "    #hard-code new features #TODO: add as param accessible outside of functions.py\n",
    "    features = ['precip_5min:mm', 'rain_water:kgm2', 'prob_rime:p', 't_1000hPa:K', 'visibility:m',] # just this 7 nov # 'snow_water:kgm2'\n",
    "               # 'clear_sky_energy_1h:J','clear_sky_rad:W', 'direct_rad:W', 'direct_rad_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'total_cloud_cover:p', 'sun_elevation:d'] # added for 8 nov\n",
    "    \n",
    "    # Ensure the 'time' column is datetime and set as index\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True, drop=False)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Calculate rolling averages for the specified features\n",
    "    for feature in features:\n",
    "        rolling_feature_name = f\"{feature}_rolling_avg_{window_size}\"\n",
    "        df[rolling_feature_name] = df[feature].rolling(window=window_size).mean()\n",
    "\n",
    "    # Handle missing data if necessary\n",
    "    df.fillna(method='bfill', inplace=True)  # Forward fill\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_train_targets(data):\n",
    "    \"\"\"Sepperate out features from the training data\"\"\"\n",
    "    targets = data[\"pv_measurement\"]\n",
    "    X_train = data.drop(columns=[\"pv_measurement\"])\n",
    "    return X_train, targets\n",
    "\n",
    "\n",
    "def get_test_data(mean=False, mean_stats=False, roll_avg=False):\n",
    "    \"\"\"Parse the test data, getting the data that has a kaggle submission id for all locations\"\"\"\n",
    "\n",
    "    # --- Check if files exist ---\n",
    "    file_paths = [\n",
    "        'data/A/X_test_estimated.parquet',\n",
    "        'data/B/X_test_estimated.parquet',\n",
    "        'data/C/X_test_estimated.parquet',\n",
    "        'data/test.csv'\n",
    "    ]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        check_file_exists(file_path)\n",
    "\n",
    "    # --- load all test data from file ---\n",
    "    X_test_estimated_a = pd.read_parquet('data/A/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_test_estimated_b = pd.read_parquet('data/B/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_test_estimated_c = pd.read_parquet('data/C/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    # --- get hourly and rename ---\n",
    "    if mean:\n",
    "        X_test_estimated_a = get_hourly_mean(X_test_estimated_a)\n",
    "        X_test_estimated_b = get_hourly_mean(X_test_estimated_b)\n",
    "        X_test_estimated_c = get_hourly_mean(X_test_estimated_c)\n",
    "    elif mean_stats:\n",
    "        X_test_estimated_a = get_hourly_stats(X_test_estimated_a)\n",
    "        X_test_estimated_b = get_hourly_stats(X_test_estimated_b)\n",
    "        X_test_estimated_c = get_hourly_stats(X_test_estimated_c)\n",
    "    else:\n",
    "        # X_test_estimated_a = get_hourly(X_test_estimated_a)\n",
    "        # X_test_estimated_b = get_hourly(X_test_estimated_b)\n",
    "        # X_test_estimated_c = get_hourly(X_test_estimated_c)\n",
    "        pass\n",
    "\n",
    "    X_test_estimated_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_test_estimated_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_test_estimated_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "\n",
    "    X_test_estimated_a[\"estimated_flag\"] = 1\n",
    "    X_test_estimated_b[\"estimated_flag\"] = 1\n",
    "    X_test_estimated_c[\"estimated_flag\"] = 1\n",
    "\n",
    "    # --- load kaggle submission data ---\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test[\"time\"] = pd.to_datetime(test[\"time\"]) # convert \"time\" to datetime format to facilitate merge\n",
    "    kaggle_submission_a = test[test[\"location\"]==\"A\"]\n",
    "    kaggle_submission_b = test[test[\"location\"]==\"B\"]\n",
    "    kaggle_submission_c = test[test[\"location\"]==\"C\"]\n",
    "\n",
    "    # --- get only the test data with a corresponding kaggle submission id ---\n",
    "    X_test_a = pd.merge(X_test_estimated_a, kaggle_submission_a, on=\"time\", how=\"right\")\n",
    "    X_test_b = pd.merge(X_test_estimated_b, kaggle_submission_b, on=\"time\", how=\"right\")\n",
    "    X_test_c = pd.merge(X_test_estimated_c, kaggle_submission_c, on=\"time\", how=\"right\")\n",
    "\n",
    "    if roll_avg:\n",
    "        X_test_a = rolling_average(X_test_a)\n",
    "        X_test_b = rolling_average(X_test_b)\n",
    "        X_test_c = rolling_average(X_test_c)\n",
    "\n",
    "    return X_test_a, X_test_b, X_test_c\n",
    "\n",
    "\n",
    "def prepare_submission(X_test_a, X_test_b, X_test_c, pred_a, pred_b, pred_c):\n",
    "    \"\"\"Parses the test data and predictions into a single df in kaggle submission format\"\"\"\n",
    "    \n",
    "    submission_a = X_test_a.copy()\n",
    "    submission_b = X_test_b.copy()\n",
    "    submission_c = X_test_c.copy()\n",
    "\n",
    "    submission_a[\"prediction\"] = pred_a\n",
    "    submission_b[\"prediction\"] = pred_b\n",
    "    submission_c[\"prediction\"] = pred_c\n",
    "\n",
    "    submission = pd.concat([submission_a, submission_b, submission_c])\n",
    "\n",
    "    submission = submission[[\"id\", \"prediction\"]]\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Featureadder class for the sklearn pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adds features.\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def cyclic_encoding(self, df):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['normalized_time'] = (df['time'].dt.hour + df['time'].dt.minute / 60 + df['time'].dt.second / 3600) / 24.0\n",
    "        df['sine_encoded'] = np.sin(2 * np.pi * df['normalized_time'])\n",
    "        df['cosine_encoded'] = np.cos(2 * np.pi * df['normalized_time'])\n",
    "\n",
    "        month = df['time'].dt.month\n",
    "        df['sine_encoded_month'] = np.sin(2 * np.pi * month)\n",
    "        df['cosine_encoded_month'] = np.cos(2 * np.pi * month)\n",
    "\n",
    "        df.drop('normalized_time', axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # # add moth\n",
    "        # X_copy['month'] = X_copy['time'].apply(lambda x: x.month)\n",
    "\n",
    "        # # add hour\n",
    "        # X_copy['hour'] = X_copy['time'].apply(lambda x: x.hour)\n",
    "\n",
    "        X_copy = self.cyclic_encoding(X_copy)\n",
    "\n",
    "        # -- additive effects:\n",
    "        X_copy[\"sun_rad_1\"] = (X_copy['sun_azimuth:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        X_copy[\"sun_rad_2\"] = (X_copy['sun_elevation:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        #X_copy[\"sun_wind_1\"] = (X_copy['wind_speed_10m:ms'] * X_copy['direct_rad:W']) / 1000\n",
    "        X_copy[\"sun_wind_2\"] = (X_copy['wind_speed_10m:ms'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy[\"temp_sun\"] = (X_copy['t_1000hPa:K'] * X_copy['sun_azimuth:d'])/1000\n",
    "        X_copy[\"rad_day_1\"] = (X_copy['is_day:idx'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy['mult_coulds'] = (X_copy['clear_sky_rad:W'] * X_copy['cloud_base_agl:m']) / 100000\n",
    "\n",
    "        #X_copy[\"dirrad_airdensity\"] = (X_copy['direct_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        #X_copy[\"diffrad_airdensity\"] = (X_copy['diffuse_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        # X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        # X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run pipeline, prepare submission, and write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from functions import load_data, get_train_targets, get_test_data, prepare_submission, remove_ouliers\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import catboost as cb\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#from featureadder import FeatureAdder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data_a, data_b, data_c = load_data(mean_stats=True, remove_out=True, roll_avg=True)\n",
    "\n",
    "X_train_a, targets_a = get_train_targets(data_a)\n",
    "X_train_b, targets_b = get_train_targets(data_b)\n",
    "X_train_c, targets_c = get_train_targets(data_c)\n",
    "\n",
    "X_test_a, X_test_b, X_test_c = get_test_data(mean_stats=True, roll_avg=True)\n",
    "\n",
    "\n",
    "drop_cols = ['time', 'elevation:m', 'fresh_snow_1h:cm', 'ceiling_height_agl:m', 'snow_density:kgm3', \n",
    "             'wind_speed_w_1000hPa:ms', 'snow_drift:idx', 'fresh_snow_3h:cm', 'is_in_shadow:idx', 'dew_or_rime:idx', 'fresh_snow_6h:cm', 'prob_rime:p'] # this second line is columns with feature importance == 0\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns from the data.\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.drop_cols)\n",
    "\n",
    "data_process_pipeline = Pipeline([\n",
    "    ('add_features', FeatureAdder()),\n",
    "    ('drop_cols', ColumnDropper(drop_cols=drop_cols)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standar', StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "base_modelsA = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\", border_count=157, depth=13, iterations=828, l2_leaf_reg=7.677745179031975, learning_rate=0.012997359346271088)), #andrea gjør søk\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True, depth=10)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\", colsample_bytree=0.588602113426499, max_depth=12, n_estimators=500, reg_alpha=1e-09, reg_lambda=0.0001, xee=0.588602113426499, learning_rate=0.023222800065583988,gboost__subsample=0.4665774062657444)), #Taheera gjør søk\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('xgb_reg3', XGBRegressor(random_state=16, eval_metric=\"mae\")),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "    #('cat_boost4', cb.CatBoostRegressor(random_state=32, silent=True, objective=\"MAE\", depth=10)), #lagt til\n",
    "]\n",
    "\n",
    "base_modelsB = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\", depth=10)),\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True, depth=10)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\")),\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "]\n",
    "\n",
    "base_modelsC = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\", depth=10)),\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True, depth=10)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\")),\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "]\n",
    "\n",
    "# Define meta-learner\n",
    "meta_learnerA = LinearRegression()\n",
    "meta_learnerB = LinearRegression()\n",
    "meta_learnerC = LinearRegression()\n",
    "\n",
    "# Create the stacking regressor\n",
    "stacked_modelA = StackingRegressor(estimators=base_modelsA, final_estimator=meta_learnerA)\n",
    "stacked_modelB = StackingRegressor(estimators=base_modelsB, final_estimator=meta_learnerB)\n",
    "stacked_modelC = StackingRegressor(estimators=base_modelsC, final_estimator=meta_learnerC)\n",
    "\n",
    "modelA_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelA)\n",
    "])\n",
    "\n",
    "modelB_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelB)\n",
    "])\n",
    "\n",
    "modelC_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelC)\n",
    "])\n",
    "\n",
    "print(\"training location A model\")\n",
    "modelA_pipeline.fit(X_train_a, targets_a)\n",
    "pred_a = modelA_pipeline.predict(X_test_a.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "print(\"training location B model\")\n",
    "modelB_pipeline.fit(X_train_b, targets_b)\n",
    "pred_b = modelB_pipeline.predict(X_test_b.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "print(\"training location C model\")\n",
    "modelC_pipeline.fit(X_train_c, targets_c)\n",
    "pred_c = modelC_pipeline.predict(X_test_c.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "submission = prepare_submission(X_test_a, X_test_b, X_test_c, pred_a, pred_b, pred_c)\n",
    "submission['prediction'] = submission['prediction'].apply(lambda x: 0 if x < 0.1 else x)\n",
    "\n",
    "submission.to_csv('submissions/11_nov_1055.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
