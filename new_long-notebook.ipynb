{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying a new approach to the feature selection, mainly using feature importance scores for different models.  \n",
    "Will run some pipelines for different models, for each of them select some features and do some hyperparameter tuning - then I can combine them using stacking (and each model can use different features!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "\n",
    "import data_preprocess as dp # data_preprocess.py\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get training data with all the features\n",
    "data = dp.data_preprocess(one_hot_location=True)\n",
    "\n",
    "features = ['absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm', 'is_day:idx',\n",
    "       'is_in_shadow:idx', 'msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms', 'A', 'B', 'C', 'time']\n",
    "\n",
    "X_train, targets = dp.get_training_data(data, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the different types of features (categorical, one-hot, and numerical)\n",
    "cat_cols = ['is_day:idx', 'is_in_shadow:idx','month', 'hour', 'day']\n",
    "\n",
    "one_hot_cols = ['A', 'B', 'C']\n",
    "\n",
    "num_cols = ['absolute_humidity_2m:gm3',\n",
    "       'air_density_2m:kgm3', 'ceiling_height_agl:m', 'clear_sky_energy_1h:J',\n",
    "       'clear_sky_rad:W', 'cloud_base_agl:m', 'dew_or_rime:idx',\n",
    "       'dew_point_2m:K', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'direct_rad:W',\n",
    "       'direct_rad_1h:J', 'effective_cloud_cover:p', 'elevation:m',\n",
    "       'fresh_snow_12h:cm', 'fresh_snow_1h:cm', 'fresh_snow_24h:cm',\n",
    "       'fresh_snow_3h:cm', 'fresh_snow_6h:cm','msl_pressure:hPa', 'precip_5min:mm',\n",
    "       'precip_type_5min:idx', 'pressure_100m:hPa', 'pressure_50m:hPa',\n",
    "       'prob_rime:p', 'rain_water:kgm2', 'relative_humidity_1000hPa:p',\n",
    "       'sfc_pressure:hPa', 'snow_density:kgm3', 'snow_depth:cm',\n",
    "       'snow_drift:idx', 'snow_melt_10min:mm', 'snow_water:kgm2',\n",
    "       'sun_azimuth:d', 'sun_elevation:d', 'super_cooled_liquid_water:kgm2',\n",
    "       't_1000hPa:K', 'total_cloud_cover:p', 'visibility:m',\n",
    "       'wind_speed_10m:ms', 'wind_speed_u_10m:ms', 'wind_speed_v_10m:ms',\n",
    "       'wind_speed_w_1000hPa:ms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "    [('categories', OneHotEncoder(dtype='int'), cat_cols),\n",
    "     ('numerical', MinMaxScaler(), num_cols),\n",
    "     ('one_hot_allready', 'passthrough', one_hot_cols),],\n",
    "     remainder='drop', verbose_feature_names_out=True)\n",
    "\n",
    "class TimeFeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adds the features month and hour to the data\"\"\"\n",
    "\n",
    "    def __init__(self, add_features=True):\n",
    "        self.add_features = add_features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        timestamps = X[\"time\"]\n",
    "        month = timestamps.apply(lambda x: x.month)\n",
    "        hour = timestamps.apply(lambda x: x.hour)\n",
    "        day = timestamps.apply(lambda x: x.day)\n",
    "\n",
    "        if self.add_features:\n",
    "            X_copy[\"month\"] = month\n",
    "            X_copy[\"hour\"] = hour\n",
    "            X_copy[\"day\"] = day\n",
    "            return X_copy\n",
    "        else:\n",
    "            return X_copy\n",
    "        \n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops the given columns from the data. Columns are indexes!!\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        if self.drop_cols is not None:\n",
    "            X_filtered = np.delete(X_copy, self.drop_cols, axis=1)\n",
    "\n",
    "        return X_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_pipeline = Pipeline([\n",
    "    ('add_features', TimeFeatureAdder()),\n",
    "    ('column_transform', column_trans),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)),\n",
    "    ('column_dropper', ColumnDropper())\n",
    "])\n",
    "\n",
    "random_forest_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('random_forest', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "grad_boost_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('grad_boost', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "sgd_regressor_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('grad_boost', SGDRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "svr_regressor_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('grad_boost', SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9387.61875852462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluate import mean_squared_error\n",
    "\n",
    "random_forest_pipeline.fit(X_train, targets)\n",
    "train_predictions = random_forest_pipeline.predict(X_train)\n",
    "mean_squared_error(train_predictions, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m all_feature_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(preprocessor\u001b[39m.\u001b[39mnamed_transformers_[\u001b[39m'\u001b[39m\u001b[39mcategories\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_feature_names_out()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(preprocessor\u001b[39m.\u001b[39mnamed_transformers_[\u001b[39m'\u001b[39m\u001b[39mnumerical\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget_feature_names_out()) \u001b[39m+\u001b[39m one_hot_cols\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# get the feature importance\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m feature_importances \u001b[39m=\u001b[39m random_forest_model\u001b[39m.\u001b[39;49mfeature_importances_\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m feature_importance_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame({\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFeature\u001b[39m\u001b[39m'\u001b[39m: all_feature_names,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m: feature_importances\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m })\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/andreastallvik/Projects/tdt4173-project/new_long-notebook.ipynb#X11sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m feature_importance_df \u001b[39m=\u001b[39m feature_importance_df\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImportance\u001b[39m\u001b[39m'\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Projects/tdt4173-project/venv/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:625\u001b[0m, in \u001b[0;36mBaseForest.feature_importances_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    605\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeature_importances_\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    606\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[39m    The impurity-based feature importances.\u001b[39;00m\n\u001b[1;32m    608\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[39m        array of zeros.\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 625\u001b[0m     check_is_fitted(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    627\u001b[0m     all_importances \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs, prefer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthreads\u001b[39m\u001b[39m\"\u001b[39m)(\n\u001b[1;32m    628\u001b[0m         delayed(\u001b[39mgetattr\u001b[39m)(tree, \u001b[39m\"\u001b[39m\u001b[39mfeature_importances_\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    629\u001b[0m         \u001b[39mfor\u001b[39;00m tree \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m tree\u001b[39m.\u001b[39mtree_\u001b[39m.\u001b[39mnode_count \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    631\u001b[0m     )\n\u001b[1;32m    633\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m all_importances:\n",
      "File \u001b[0;32m~/Projects/tdt4173-project/venv/lib/python3.10/site-packages/sklearn/utils/validation.py:1461\u001b[0m, in \u001b[0;36mcheck_is_fitted\u001b[0;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[1;32m   1458\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is not an estimator instance.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator))\n\u001b[1;32m   1460\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[0;32m-> 1461\u001b[0m     \u001b[39mraise\u001b[39;00m NotFittedError(msg \u001b[39m%\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(estimator)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m})\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "# get the 30 least important features\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# get the rf model from the pipeline\n",
    "random_forest_model = random_forest_pipeline.named_steps['random_forest']\n",
    "\n",
    "# get the column transformer and feature names\n",
    "preprocessor = data_process_pipeline.named_steps['column_transform']\n",
    "all_feature_names = list(preprocessor.named_transformers_['categories'].get_feature_names_out()) + list(preprocessor.named_transformers_['numerical'].get_feature_names_out()) + one_hot_cols\n",
    "\n",
    "# get the feature importance\n",
    "feature_importances = random_forest_model.feature_importances_\n",
    "\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': all_feature_names,\n",
    "    'Importance': feature_importances\n",
    "})\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "rf_least_important_features = list(feature_importance_df.tail(30)[\"Feature\"])\n",
    "rf_least_important_feature_index = list(feature_importance_df.tail(30).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[21,\n",
       " 3,\n",
       " 85,\n",
       " 116,\n",
       " 100,\n",
       " 89,\n",
       " 20,\n",
       " 4,\n",
       " 33,\n",
       " 102,\n",
       " 34,\n",
       " 15,\n",
       " 19,\n",
       " 88,\n",
       " 95,\n",
       " 113,\n",
       " 77,\n",
       " 86,\n",
       " 35,\n",
       " 18,\n",
       " 36,\n",
       " 37,\n",
       " 0,\n",
       " 17,\n",
       " 38,\n",
       " 39,\n",
       " 1,\n",
       " 16,\n",
       " 101,\n",
       " 99]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_least_important_feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hour_5',\n",
       " 'is_in_shadow:idx_1.0',\n",
       " 'fresh_snow_12h:cm',\n",
       " 'C',\n",
       " 'snow_depth:cm',\n",
       " 'fresh_snow_6h:cm',\n",
       " 'hour_4',\n",
       " 'month_1',\n",
       " 'hour_17',\n",
       " 'snow_melt_10min:mm',\n",
       " 'hour_18',\n",
       " 'month_12',\n",
       " 'hour_3',\n",
       " 'fresh_snow_3h:cm',\n",
       " 'prob_rime:p',\n",
       " 'wind_speed_w_1000hPa:ms',\n",
       " 'dew_or_rime:idx',\n",
       " 'fresh_snow_1h:cm',\n",
       " 'hour_19',\n",
       " 'hour_2',\n",
       " 'hour_20',\n",
       " 'hour_21',\n",
       " 'is_day:idx_0.0',\n",
       " 'hour_1',\n",
       " 'hour_22',\n",
       " 'hour_23',\n",
       " 'is_day:idx_1.0',\n",
       " 'hour_0',\n",
       " 'snow_drift:idx',\n",
       " 'snow_density:kgm3']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_least_important_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second try - now having dropped the 30 least important columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the drop_cols parameter for the ColumnDropper object in data process\n",
    "random_forest_pipeline.named_steps['data_process'].named_steps['column_dropper'].drop_cols = rf_least_important_feature_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9367.48673555216"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_pipeline.fit(X_train, targets)\n",
    "train_predictions = random_forest_pipeline.predict(X_train)\n",
    "mean_squared_error(train_predictions, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRADIENT BOOSTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOCHASTIC GRADIENT DESCENT REGRESSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SUPPORT VECTOR MACHINE REGRESSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
