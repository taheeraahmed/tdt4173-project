{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for:\n",
    "- loading data\n",
    "- parsing submission\n",
    "- ...\n",
    "\n",
    "NOTE: all functions file should be pasted into the long notebook before submission.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def check_file_exists(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"File {file_path} does not exist.\")\n",
    "    \n",
    "\n",
    "def load_data(mean=False, roll_avg=False, remove_out=False):\n",
    "    \"\"\"Loads data, drops rows that have missing values for the target variable.\"\"\"\n",
    "\n",
    "    # --- Check if files exist ---\n",
    "    file_paths = [\n",
    "        'data/A/train_targets.parquet',\n",
    "        'data/B/train_targets.parquet',\n",
    "        'data/C/train_targets.parquet',\n",
    "        'data/A/X_train_estimated.parquet',\n",
    "        'data/B/X_train_estimated.parquet',\n",
    "        'data/C/X_train_estimated.parquet',\n",
    "        'data/A/X_train_observed.parquet',\n",
    "        'data/B/X_train_observed.parquet',\n",
    "        'data/C/X_train_observed.parquet',\n",
    "    ]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        check_file_exists(file_path)\n",
    "\n",
    "    # ---- load data from files ----\n",
    "    train_a = pd.read_parquet('data/A/train_targets.parquet')\n",
    "    train_b = pd.read_parquet('data/B/train_targets.parquet')\n",
    "    train_c = pd.read_parquet('data/C/train_targets.parquet')\n",
    "\n",
    "    X_train_observed_a = pd.read_parquet('data/A/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_b = pd.read_parquet('data/B/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_observed_c = pd.read_parquet('data/C/X_train_observed.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    X_train_estimated_a = pd.read_parquet('data/A/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_b = pd.read_parquet('data/B/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_train_estimated_c = pd.read_parquet('data/C/X_train_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    # --- get features for each hour before concatinating ---\n",
    "    if mean:\n",
    "        X_train_observed_a = get_hourly_mean(X_train_observed_a)\n",
    "        X_train_observed_b = get_hourly_mean(X_train_observed_b)\n",
    "        X_train_observed_c = get_hourly_mean(X_train_observed_c)\n",
    "\n",
    "        X_train_estimated_a = get_hourly_mean(X_train_estimated_a)\n",
    "        X_train_estimated_b = get_hourly_mean(X_train_estimated_b)\n",
    "        X_train_estimated_c = get_hourly_mean(X_train_estimated_c)\n",
    "\n",
    "    else:\n",
    "        X_train_observed_a = get_hourly(X_train_observed_a)\n",
    "        X_train_observed_b = get_hourly(X_train_observed_b)\n",
    "        X_train_observed_c = get_hourly(X_train_observed_c)\n",
    "\n",
    "        X_train_estimated_a = get_hourly(X_train_estimated_a)\n",
    "        X_train_estimated_b = get_hourly(X_train_estimated_b)\n",
    "        X_train_estimated_c = get_hourly(X_train_estimated_c)\n",
    "\n",
    "    X_train_observed_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_observed_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_train_estimated_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "\n",
    "    X_train_observed_a[\"estimated_flag\"] = 0\n",
    "    X_train_observed_b[\"estimated_flag\"] = 0\n",
    "    X_train_observed_c[\"estimated_flag\"] = 0\n",
    "    X_train_estimated_a[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_b[\"estimated_flag\"] = 1\n",
    "    X_train_estimated_c[\"estimated_flag\"] = 1\n",
    "\n",
    "    # --- merge observed and estimated data with target data, lining up time-stamps correctly ----\n",
    "    train_obs_a = pd.merge(train_a, X_train_observed_a, on='time', how='inner')\n",
    "    train_obs_b = pd.merge(train_b, X_train_observed_b, on='time', how='inner') # NOTE: 4 missing values for target\n",
    "    train_obs_c = pd.merge(train_c, X_train_observed_c, on='time', how='inner') # NOTE: 6059 missing values for target\n",
    "\n",
    "    train_est_a = pd.merge(train_a, X_train_estimated_a, on='time', how='inner')\n",
    "    train_est_b = pd.merge(train_b, X_train_estimated_b, on='time', how='inner')\n",
    "    train_est_c = pd.merge(train_c, X_train_estimated_c, on='time', how='inner')\n",
    "\n",
    "    data_a = pd.concat([train_obs_a, train_est_a], axis=0, ignore_index=True)\n",
    "    data_b = pd.concat([train_obs_b, train_est_b], axis=0, ignore_index=True)\n",
    "    data_c = pd.concat([train_obs_c, train_est_c], axis=0, ignore_index=True)\n",
    "\n",
    "    # remove rows that the target value is missing from since they will not be useful in model training\n",
    "    data_a = data_a.dropna(subset=['pv_measurement'])\n",
    "    data_b = data_b.dropna(subset=['pv_measurement'])\n",
    "    data_c = data_c.dropna(subset=['pv_measurement'])\n",
    "\n",
    "    # add columnns for rolling average\n",
    "    if roll_avg:\n",
    "        data_a = rolling_average(data_a)\n",
    "        data_b = rolling_average(data_b)\n",
    "        data_c = rolling_average(data_c)\n",
    "\n",
    "    if remove_out:\n",
    "        data_a = remove_ouliers(data_a)\n",
    "        data_b = remove_ouliers(data_b, remove_b_outliers=True)\n",
    "        data_c = remove_ouliers(data_c)\n",
    "\n",
    "    return data_a, data_b, data_c\n",
    "\n",
    "\n",
    "def remove_ouliers(data, remove_b_outliers = False):\n",
    "    \"\"\"Removes datapoints that have been static over long stretches (likely due to sensor error!).\"\"\"\n",
    "\n",
    "    threshold = 0.01\n",
    "    window_size = 24\n",
    "\n",
    "    # Calculate standard deviation for each window\n",
    "    std_dev = data['pv_measurement'].rolling(window=window_size, min_periods=1).std()\n",
    "\n",
    "    # Identify constant stretches and create a mask to filter out these points\n",
    "    constant_mask = std_dev < threshold\n",
    "\n",
    "    # Filter out constant stretches from the data\n",
    "    filtered_data = data[~constant_mask]\n",
    "\n",
    "    if remove_b_outliers:\n",
    "        \"removing some extra outliers\"\n",
    "        # Remove rows where pv_measurement > 100 and diffuse_rad:W < 30\n",
    "        filtered_data = filtered_data[~((filtered_data[\"pv_measurement\"] > 100) & (filtered_data[\"diffuse_rad:W\"] < 30))]\n",
    "\n",
    "        # Remove rows where pv_measurement > 200 and diffuse_rad:W < 40\n",
    "        filtered_data = filtered_data[~((filtered_data[\"pv_measurement\"] > 200) & (filtered_data[\"diffuse_rad:W\"] < 40))]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "\n",
    "def get_hourly(df):\n",
    "    \n",
    "    df[\"minute\"] = df[\"time\"].dt.minute\n",
    "\n",
    "    min_vals = df[\"minute\"].unique()\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "    for value in min_vals:\n",
    "        filtered_data = df[df['minute'] == value].copy()\n",
    "        filtered_data.drop(columns=['minute'], inplace=True)\n",
    "        filtered_data.columns = [f'{col}_{value}' for col in filtered_data.columns]\n",
    "        filtered_data[\"time_hour\"] = filtered_data[\"time_\"+str(value)].apply(lambda x: x.floor('H'))\n",
    "        df_list.append(filtered_data)\n",
    "\n",
    "    # merge df's on hourly time\n",
    "    merged_df = pd.merge(df_list[0], df_list[1], on=\"time_hour\")\n",
    "    for df in df_list[2:]:\n",
    "        merged_df = pd.merge(merged_df, df, on=\"time_hour\")\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "def get_hourly_mean(df):\n",
    "    \"\"\"Returns a dataframe in which \"\"\"\n",
    "    \n",
    "    # get a column for the start hour\n",
    "    df[\"time_hour\"] = df[\"time\"].apply(lambda x: x.floor('H'))\n",
    "    \n",
    "    # get the mean value for the entire hour\n",
    "    mean_df = df.groupby('time_hour').agg('mean').reset_index()\n",
    "\n",
    "    return mean_df\n",
    "\n",
    "def rolling_average(df, window_size=24,features=['clear_sky_energy_1h:J','clear_sky_rad:W', 'direct_rad:W', 'direct_rad_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'total_cloud_cover:p', 'sun_elevation:d']):\n",
    "    \n",
    "    #hard-code new features #TODO: add as param accessible outside of functions.py\n",
    "    features = ['precip_5min:mm', 'rain_water:kgm2', 'prob_rime:p', 't_1000hPa:K', 'visibility:m',] # just this 7 nov # 'snow_water:kgm2'\n",
    "               # 'clear_sky_energy_1h:J','clear_sky_rad:W', 'direct_rad:W', 'direct_rad_1h:J', 'diffuse_rad:W', 'diffuse_rad_1h:J', 'total_cloud_cover:p', 'sun_elevation:d'] # added for 8 nov\n",
    "    \n",
    "    # Ensure the 'time' column is datetime and set as index\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True, drop=False)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Calculate rolling averages for the specified features\n",
    "    for feature in features:\n",
    "        rolling_feature_name = f\"{feature}_rolling_avg_{window_size}\"\n",
    "        df[rolling_feature_name] = df[feature].rolling(window=window_size).mean()\n",
    "\n",
    "    # Handle missing data if necessary\n",
    "    df.fillna(method='bfill', inplace=True)  # Forward fill\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_train_targets(data):\n",
    "    \"\"\"Sepperate out features from the training data\"\"\"\n",
    "    targets = data[\"pv_measurement\"]\n",
    "    X_train = data.drop(columns=[\"pv_measurement\"])\n",
    "    return X_train, targets\n",
    "\n",
    "\n",
    "def get_test_data(mean=False, roll_avg=False):\n",
    "    \"\"\"Parse the test data, getting the data that has a kaggle submission id for all locations\"\"\"\n",
    "\n",
    "    # --- Check if files exist ---\n",
    "    file_paths = [\n",
    "        'data/A/X_test_estimated.parquet',\n",
    "        'data/B/X_test_estimated.parquet',\n",
    "        'data/C/X_test_estimated.parquet',\n",
    "        'data/test.csv'\n",
    "    ]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        check_file_exists(file_path)\n",
    "\n",
    "    # --- load all test data from file ---\n",
    "    X_test_estimated_a = pd.read_parquet('data/A/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_test_estimated_b = pd.read_parquet('data/B/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "    X_test_estimated_c = pd.read_parquet('data/C/X_test_estimated.parquet').rename(columns={'date_forecast': 'time'})\n",
    "\n",
    "    # --- get hourly and rename ---\n",
    "    if mean:\n",
    "        X_test_estimated_a = get_hourly_mean(X_test_estimated_a)\n",
    "        X_test_estimated_b = get_hourly_mean(X_test_estimated_b)\n",
    "        X_test_estimated_c = get_hourly_mean(X_test_estimated_c)\n",
    "    else:\n",
    "        X_test_estimated_a = get_hourly(X_test_estimated_a)\n",
    "        X_test_estimated_b = get_hourly(X_test_estimated_b)\n",
    "        X_test_estimated_c = get_hourly(X_test_estimated_c)\n",
    "\n",
    "    X_test_estimated_a.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_test_estimated_b.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "    X_test_estimated_c.rename(columns={\"time_hour\": \"time\"}, inplace=True)\n",
    "\n",
    "    X_test_estimated_a[\"estimated_flag\"] = 1\n",
    "    X_test_estimated_b[\"estimated_flag\"] = 1\n",
    "    X_test_estimated_c[\"estimated_flag\"] = 1\n",
    "\n",
    "    # --- load kaggle submission data ---\n",
    "    test = pd.read_csv('data/test.csv')\n",
    "    test[\"time\"] = pd.to_datetime(test[\"time\"]) # convert \"time\" to datetime format to facilitate merge\n",
    "    kaggle_submission_a = test[test[\"location\"]==\"A\"]\n",
    "    kaggle_submission_b = test[test[\"location\"]==\"B\"]\n",
    "    kaggle_submission_c = test[test[\"location\"]==\"C\"]\n",
    "\n",
    "    # --- get only the test data with a corresponding kaggle submission id ---\n",
    "    X_test_a = pd.merge(X_test_estimated_a, kaggle_submission_a, on=\"time\", how=\"right\")\n",
    "    X_test_b = pd.merge(X_test_estimated_b, kaggle_submission_b, on=\"time\", how=\"right\")\n",
    "    X_test_c = pd.merge(X_test_estimated_c, kaggle_submission_c, on=\"time\", how=\"right\")\n",
    "\n",
    "    if roll_avg:\n",
    "        X_test_a = rolling_average(X_test_a)\n",
    "        X_test_b = rolling_average(X_test_b)\n",
    "        X_test_c = rolling_average(X_test_c)\n",
    "\n",
    "    return X_test_a, X_test_b, X_test_c\n",
    "\n",
    "\n",
    "def prepare_submission(X_test_a, X_test_b, X_test_c, pred_a, pred_b, pred_c):\n",
    "    \"\"\"Parses the test data and predictions into a single df in kaggle submission format\"\"\"\n",
    "    \n",
    "    submission_a = X_test_a.copy()\n",
    "    submission_b = X_test_b.copy()\n",
    "    submission_c = X_test_c.copy()\n",
    "\n",
    "    submission_a[\"prediction\"] = pred_a\n",
    "    submission_b[\"prediction\"] = pred_b\n",
    "    submission_c[\"prediction\"] = pred_c\n",
    "\n",
    "    submission = pd.concat([submission_a, submission_b, submission_c])\n",
    "\n",
    "    submission = submission[[\"id\", \"prediction\"]]\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adds features.\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def cyclic_encoding(self, df):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['normalized_time'] = (df['time'].dt.hour + df['time'].dt.minute / 60 + df['time'].dt.second / 3600) / 24.0\n",
    "        df['sine_encoded'] = np.sin(2 * np.pi * df['normalized_time'])\n",
    "        df['cosine_encoded'] = np.cos(2 * np.pi * df['normalized_time'])\n",
    "\n",
    "        month = df['time'].dt.month\n",
    "        df['sine_encoded_month'] = np.sin(2 * np.pi * month)\n",
    "        df['cosine_encoded_month'] = np.cos(2 * np.pi * month)\n",
    "\n",
    "        df.drop('normalized_time', axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # # add moth\n",
    "        # X_copy['month'] = X_copy['time'].apply(lambda x: x.month)\n",
    "\n",
    "        # # add hour\n",
    "        # X_copy['hour'] = X_copy['time'].apply(lambda x: x.hour)\n",
    "\n",
    "        X_copy = self.cyclic_encoding(X_copy)\n",
    "\n",
    "        # -- additive effects:\n",
    "        X_copy[\"sun_rad_1\"] = (X_copy['sun_azimuth:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        X_copy[\"sun_rad_2\"] = (X_copy['sun_elevation:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        #X_copy[\"sun_wind_1\"] = (X_copy['wind_speed_10m:ms'] * X_copy['direct_rad:W']) / 1000\n",
    "        X_copy[\"sun_wind_2\"] = (X_copy['wind_speed_10m:ms'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy[\"temp_sun\"] = (X_copy['t_1000hPa:K'] * X_copy['sun_azimuth:d'])/1000\n",
    "        X_copy[\"rad_day_1\"] = (X_copy['is_day:idx'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy['mult_coulds'] = (X_copy['clear_sky_rad:W'] * X_copy['cloud_base_agl:m']) / 100000\n",
    "\n",
    "        #X_copy[\"dirrad_airdensity\"] = (X_copy['direct_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        #X_copy[\"diffrad_airdensity\"] = (X_copy['diffuse_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        # X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        # X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functions import load_data, get_train_targets, get_test_data, prepare_submission, remove_ouliers\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import catboost as cb\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from featureadder import FeatureAdder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Suppress all FutureWarnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "data_a, data_b, data_c = load_data(mean=True, remove_out=True, roll_avg=True)\n",
    "\n",
    "X_train_a, targets_a = get_train_targets(data_a)\n",
    "X_train_b, targets_b = get_train_targets(data_b)\n",
    "X_train_c, targets_c = get_train_targets(data_c)\n",
    "\n",
    "X_test_a, X_test_b, X_test_c = get_test_data(mean=True, roll_avg=True)\n",
    "\n",
    "\n",
    "drop_cols = ['time', 'elevation:m', 'fresh_snow_1h:cm', 'ceiling_height_agl:m', 'snow_density:kgm3', \n",
    "             'wind_speed_w_1000hPa:ms', 'snow_drift:idx', 'fresh_snow_3h:cm', 'is_in_shadow:idx', 'dew_or_rime:idx', 'fresh_snow_6h:cm', 'prob_rime:p'] # this second line is columns with feature importance == 0\n",
    "\n",
    "class ColumnDropper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Drops columns from the data.\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "        return X_copy.drop(columns=self.drop_cols)\n",
    "\n",
    "data_process_pipeline = Pipeline([\n",
    "    ('add_features', FeatureAdder()),\n",
    "    ('drop_cols', ColumnDropper(drop_cols=drop_cols)),\n",
    "    ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "    ('standar', StandardScaler()),\n",
    "])\n",
    "\n",
    "\n",
    "base_modelsA = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\")),\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\", colsample_bytree=0.6396572695576039, gamma=0.3122839968432516, learning_rate=0.030962102428926407, max_depth=8, n_estimators=258, subsample=0.9202906451199873)),\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('xgb_reg3', XGBRegressor(random_state=16, eval_metric=\"mae\")),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "]\n",
    "\n",
    "base_modelsB = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\")),\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\")),\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "]\n",
    "\n",
    "base_modelsC = [\n",
    "    ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\")),\n",
    "    ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True)),\n",
    "    ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\")),\n",
    "    ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "    ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "]\n",
    "\n",
    "# Define meta-learner\n",
    "meta_learnerA = LinearRegression()\n",
    "meta_learnerB = LinearRegression()\n",
    "meta_learnerC = LinearRegression()\n",
    "\n",
    "# Create the stacking regressor\n",
    "stacked_modelA = StackingRegressor(estimators=base_modelsA, final_estimator=meta_learnerA)\n",
    "stacked_modelB = StackingRegressor(estimators=base_modelsB, final_estimator=meta_learnerB)\n",
    "stacked_modelC = StackingRegressor(estimators=base_modelsC, final_estimator=meta_learnerC)\n",
    "\n",
    "modelA_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelA)\n",
    "])\n",
    "\n",
    "modelB_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelB)\n",
    "])\n",
    "\n",
    "modelC_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelC)\n",
    "])\n",
    "\n",
    "print(\"training location A model\")\n",
    "modelA_pipeline.fit(X_train_a, targets_a)\n",
    "pred_a = modelA_pipeline.predict(X_test_a.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "print(\"training location B model\")\n",
    "modelB_pipeline.fit(X_train_b, targets_b)\n",
    "pred_b = modelB_pipeline.predict(X_test_b.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "print(\"training location C model\")\n",
    "modelC_pipeline.fit(X_train_c, targets_c)\n",
    "pred_c = modelC_pipeline.predict(X_test_c.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "submission = prepare_submission(X_test_a, X_test_b, X_test_c, pred_a, pred_b, pred_c)\n",
    "submission['prediction'] = submission['prediction'].apply(lambda x: 0 if x < 0.05 else x)\n",
    "\n",
    "submission.to_csv('submissions/9_nov_2217.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
