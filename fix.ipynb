{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils.read_data import load_data, get_train_targets, get_test_data, prepare_submission\n",
    "from utils.generate_run_name import generate_run_name\n",
    "from utils.data_pipeline import ColumnDropper\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Adds features.\"\"\"\n",
    "\n",
    "    def __init__(self, drop_cols = []):\n",
    "        self.drop_cols = drop_cols\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def cyclic_encoding(self, df):\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df['normalized_time'] = (df['time'].dt.hour + df['time'].dt.minute / 60 + df['time'].dt.second / 3600) / 24.0\n",
    "        df['sine_encoded'] = np.sin(2 * np.pi * df['normalized_time'])\n",
    "        df['cosine_encoded'] = np.cos(2 * np.pi * df['normalized_time'])\n",
    "\n",
    "        month = df['time'].dt.month\n",
    "        df['sine_encoded_month'] = np.sin(2 * np.pi * month)\n",
    "        df['cosine_encoded_month'] = np.cos(2 * np.pi * month)\n",
    "\n",
    "        df.drop('normalized_time', axis=1, inplace=True)\n",
    "        return df\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_copy = X.copy()\n",
    "\n",
    "        # # add moth\n",
    "        # X_copy['month'] = X_copy['time'].apply(lambda x: x.month)\n",
    "\n",
    "        # # add hour\n",
    "        # X_copy['hour'] = X_copy['time'].apply(lambda x: x.hour)\n",
    "\n",
    "        X_copy = self.cyclic_encoding(X_copy)\n",
    "\n",
    "        # -- additive effects:\n",
    "        X_copy[\"sun_rad_1\"] = (X_copy['sun_azimuth:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        X_copy[\"sun_rad_2\"] = (X_copy['sun_elevation:d'] * X_copy['direct_rad:W']) / 1000000\n",
    "        #X_copy[\"sun_wind_1\"] = (X_copy['wind_speed_10m:ms'] * X_copy['direct_rad:W']) / 1000\n",
    "        X_copy[\"sun_wind_2\"] = (X_copy['wind_speed_10m:ms'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy[\"temp_sun\"] = (X_copy['t_1000hPa:K'] * X_copy['sun_azimuth:d'])/1000\n",
    "        X_copy[\"rad_day_1\"] = (X_copy['is_day:idx'] * X_copy['diffuse_rad:W']) / 1000\n",
    "        X_copy['mult_coulds'] = (X_copy['clear_sky_rad:W'] * X_copy['cloud_base_agl:m']) / 100000\n",
    "\n",
    "        #X_copy[\"dirrad_airdensity\"] = (X_copy['direct_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        #X_copy[\"diffrad_airdensity\"] = (X_copy['diffuse_rad:W'] * X_copy['air_density_2m:kgm3'])/1000 #unsure\n",
    "        X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        # X_copy[\"ratio_rad1\"] = (X_copy['direct_rad:W'] / X_copy['diffuse_rad:W']) # good one!\n",
    "        # X_copy[\"temp_rad_1\"] = (X_copy['t_1000hPa:K'] * X_copy['direct_rad:W'])/1000\n",
    "\n",
    "        return X_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a, data_b, data_c = load_data(mean=True, remove_out=True, roll_avg=True)\n",
    "\n",
    "X_train_a, targets_a = get_train_targets(data_a)\n",
    "X_train_b, targets_b = get_train_targets(data_b)\n",
    "X_train_c, targets_c = get_train_targets(data_c)\n",
    "\n",
    "X_test_a, X_test_b, X_test_c = get_test_data(mean=True, roll_avg=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['time', 'elevation:m', 'fresh_snow_1h:cm', 'ceiling_height_agl:m', 'snow_density:kgm3', \n",
    "                'wind_speed_w_1000hPa:ms', 'snow_drift:idx', 'fresh_snow_3h:cm', 'is_in_shadow:idx', 'dew_or_rime:idx', 'fresh_snow_6h:cm', 'prob_rime:p'] # this second line is columns with feature importance == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_process_pipeline = Pipeline([\n",
    "        ('add_features', FeatureAdder()),\n",
    "        ('drop_cols', ColumnDropper(drop_cols=drop_cols)),\n",
    "        ('imputer', SimpleImputer(missing_values=np.nan, strategy='median')),\n",
    "        ('standar', StandardScaler()),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_modelsC = [\n",
    "        ('cat_boost1', cb.CatBoostRegressor(random_state=1, silent=True, objective=\"MAE\", depth=10)),\n",
    "        ('cat_boost2', cb.CatBoostRegressor(random_state=2, silent=True, depth=10)),\n",
    "        ('xgb_reg1', XGBRegressor(random_state=12, eval_metric=\"mae\")),\n",
    "        ('xgb_reg2', XGBRegressor(random_state=42)),\n",
    "        ('cat_boost3', cb.CatBoostRegressor(random_state=3, silent=True)),\n",
    "    ]\n",
    "\n",
    "meta_learnerC = LinearRegression()\n",
    "stacked_modelC = StackingRegressor(estimators=base_modelsC, final_estimator=meta_learnerC)\n",
    "\n",
    "\n",
    "modelC_pipeline = Pipeline([\n",
    "    ('data_process', data_process_pipeline),\n",
    "    ('stacked_model', stacked_modelC)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelC_pipeline.fit(X_train_c, targets_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_c = modelC_pipeline.predict(X_test_c.drop(columns=[\"id\", \"prediction\", \"location\"]))\n",
    "\n",
    "#prepare_submission(_, _, X_test_c, _, _, pred_c, 'run_name')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdt4173-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
